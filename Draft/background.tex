\section{Background}
\label{sec:background}
In this section we provide work related to structural equation modeling, security practices, and measurement frameworks.

\subsection{Structural Equation Modeling}
Structural Equation Modeling (SEM) is a family of statistical techniques for testing theories by specifying models that represent predictions of that theory among plausible constructs, measured with appropriate observed variables~\cite{kline2015principles}. Suhr provides a concise online tutorial~\footnote{http://www.lexjansen.com/wuss/2006/tutorials/TUT-Suhr.pdf}. While most traditional statistical methods emphasize the modeling of individual observations, SEM emphasizes the covariances of the observed variables~\cite{hildreth2013residual}. In SEM, a model is posited that specifies the relationships among variables, resulting in systems of linear equations such that the relationships between all variables are linear (or transformably linear). In these linear equations, variables are linked by structural parameters indicating the expected (modeled) relationships between variables. Based on these equations, the population covariance matrix of the observed variables can be represented as a function of the model parameters. The population covariance matrix is derived from datasets representing a sample drawn from the population, and `estimator' algorithms are applied to the model and dataset to solve for the model parameter estimates. Hildreth ~\cite{hildreth2013residual}, Chapter 2, gives a clear presentation of the linear algebra involved in specifying and solving SEM models. 

We rely, in particular, on two features of SEM to support our investigation: the distinction between observed variables and latent variables, and the distinction between structural and measurement models.  

Observed variables are the data researchers collect to study a construct of interest. However, not every quantity we wish to measure can be measured directly. For example, in psychological studies of `general intelligence', \textit{g}, researchers examine correlations between various and multiple observed test scores and the notion, or `construct' of general intelligence. Latent variables are a tool for analyzing combinations of observed variables that correspond to hypothetical constructs, which are presumed to reflect a continuum that is not directly observable ~\cite{kline2015principles}. Researchers commonly investigate the effects of unobservables like intelligence by statistically relating covariation between observed variables to latent variables~\cite{borsboom2003theoretical}.  

%One or more observed variables may be modeled as either causing (formative) or being caused by (reflective) a latent variable. 
We adopt latent variables as a suitable tool for relating observed data to the constructs we study, for example Asset Impact, and Software Risk.

The distinction between structural and measurement models parallels the distinction between latent variables and observed variables. Structural models are representations of the latent variables and the relationships between them. Measurement models are representations of observed variables and the relationships between them.  A measurement model may be linked to a structural model to indicate relationships between observed and latent variables. Repeated studies with a single structural-measurement model combination contribute evidence for accepting or rejecting the combined models. A single structural model may be associated, serially, with a variety of measurement models, reflecting a variety of approaches to measuring a theoretical construct.  Observations of similar trends using different data sets and measurement approaches lends strength to the underlying theory~\cite{basili1999building,wohlin2000experimentation}. 

A number of software packages, including sem~\footnote{https://cran.r-project.org/web/packages/sem/index.html}, MPlus~\footnote{https://www.statmodel.com/}, and lavaan~\cite{roseel2012lavaan}, provide tools for specifying SEM models and a variety of estimation algorithms to match the characteristics of models and datasets. 

SEM studies are organized around six steps (which may be iterated over, as called for by the needs of the study): 
\begin{itemize}
\item \textit{model specification}, researchers express the hypothesized relationships between observed variables and latent variables, typically in the form of a graphical model. Each edge in the graph represents a parameter to be estimated, indicating the strength of the relationship between the nodes connected by the edge,

\item \textit{identification}, the specified model is checked against statistical theory for whether all of the model’s parameters can be estimated given the model's structure. Identification is analogous to checking whether a set of equations is solvable given their structure and the data at hand. If the original model cannot be identified, it must be revised in light of both statistical theory and the theory the researcher is expressing in the model,

\item \textit{data selection}, where data for each of the model’s observed variables is chosen. Kline ~\cite{kline2015principles} states that SEM is a large-sample technique, reporting median sample size in the literature of 200 cases, starting with a minimum of 10 observations per parameter to be estimated.  Factors that drive the need for a large sample include the size and complexity of the model to be estimated, non-normally distributed data, categorical data, and imprecisely measured data,

\item \textit{data collection}, where the selected data is obtained from the study's sources,

\item \textit{estimation}, the observed data and the model are checked for fit.  If appropriate fit is achieved, the parameter estimates can be interpreted for implications of the theorized relationships and the observed data. If appropriate fit is not achieved, the list of model changes developed during specification should be considered in re-specifying the model, and

\item \textit{reporting results}, the model, parameter estimates,  fit measures, and changes made for re-specification should be included in the report.

Examples of SEM use in software engineering and information technology include Capra et al.~\cite{capra2008empirical}, Wallace and Sheetz~\cite{wallace2014adoption}, and Gopal et al.~\cite{gopal2005impact}.
\end{itemize}

\subsection{Measurement Frameworks}
Measurement frameworks offer a foundation for aggregating study results by standardizing the collection of empirical evidence, enabling comparison of results across projects. Previous researchers~\cite{kitchenham1999towards,williams2004toward} have suggested four reasons for measurement frameworks:
\begin{itemize}
	\item To allow researchers to provide a context within which specific questions can be investigated;
	\item To help researchers understand and resolve contradictory results observed in empirical studies;
	\item to provide researchers a standard framework to assist in data collection and reporting of empirical studies in such a manner that they can be classified, understood, and replicated and to help industrial adoption of research results; and
	\item  to provide researchers a framework for categorizing empirical studies into a body of knowledge.
\end{itemize}

Williams et al.~\cite{williams2004toward} defined a measurement framework for evaluating the use of Extreme Programming (XP), XP-EF. XP-EF contains context factors to capture internal project-related variables; adherence metrics to capture XP practice use; and outcome measures to capture external project results (e.g. quality). Modeled after XP-EF, we have developed SP-EF, a measurement framework for software development security practice use. We defined a similarly structured set of measures for recording context factors, practice adherence metrics, and outcome measures, related to the use of security practices in software development.
Rudolph and Schwartz~\cite{rudolph2012critical} conducted a systematic literature review, and Morrison et al.~\cite{morrison2014mapping} produced a technical report on security metrics. We chose the context factors and outcome measures aligned with the findings of these surveys.

The Common Vulnerability Scoring System~\cite{mell2006common} (CVSS) is a framework for communicating the characteristics of vulnerabilities in information technology (IT). Our focus is on the software development process and product rather than the individual vulnerability, however we adopt the Confidentiality Requirement, Integrity Requirement, and Availability Requirement elements of CVSS as context factors.