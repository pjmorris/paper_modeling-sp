\section{Background}
\label{sec:background}
In this section we provide background on SEM, and work related to security practices, measurement frameworks, and text mining.
\subsection{Structural Equation Modeling}

\begin{quotation}
	\textit{“So long as we want to try to describe complex real-life phenomena as they
		occur in their natural settings, it seems that our chief alternatives are the
		literary essay and the path model”} - John Loehlin ~\cite{loehlin1986latent}
\end{quotation}
Pearl~\cite{pearl2012causal} defines SEM as a causal inference method with three inputs and three outputs.  The inputs are a set of causal hypotheses, a set of questions about causal relations among variables of interest, and data.  The outputs are numeric estimates of model parameters for hypothesized effects, a set of logical implications of the model, and a measure of the degree to which the testable implications of the model are supported by the data. 
SEM studies are organized around six steps~\cite{kline2015principles}: specification, identification, data selection and collection, estimation, re-specification, and reporting. In model specification, researchers express the hypothesized relationships between observed variables and latent variables, typically in the form of a graphical model. Each edge in the graph represents a parameter to be estimated, indicating the strength of the relationship between the nodes connected by the edge. In addition to nodes for the observed and latent variables, nodes representing error terms are included in typical models. During specification, a list of theoretically justified possible changes to the model should be developed, should the original model’s fit to the data be inadequate. In identification, the specified model is checked against statistical theory for whether all of the model’s parameters can be estimated. If the original model cannot be identified, it must be revised in light of both statistical theory and the theory the researcher is expressing in the model. In data selection and collection, data for each of the model’s observed variables is chosen, and collected. In estimation, the observed data and the model are checked for fit.  If appropriate fit is achieved, the parameter estimates can be interpreted for implications of the theorized relationships and the observed data. If appropriate fit is not achieved, the list of model changes developed during specification should be considered in re-specifying the model.  When reporting the results of SEM studies, the model, parameter estimates, and fit measures should be included in the report.
Examples of SEM use in software engineering and information technology include Capra et al.~\cite{capra2008empirical}, Wallace and Sheetz~\cite{wallace2014adoption}, and Gopal et al.~\cite{gopal2005impact} 
\subsection{Measurement Frameworks}
Measurement frameworks offer a foundation for aggregating study results by standardizing the collection of empirical evidence, enabling comparison of results across projects. Previous researchers~\cite{kitchenham1999towards}, ~\cite{williams2004toward} have suggested four reasons for measurement frameworks: 
• To allow researchers to provide a context within which specific questions can be investigated;
• To help researchers understand and resolve contradictory results observed in empirical studies;
• to provide researchers a standard framework to assist in data collection and reporting of empirical studies in such a manner that they can be classified, understood, and replicated and to help industrial adoption of research results; and
• to provide researchers a framework for categorizing empirical studies into a body of knowledge.
Williams et al.~\cite{williams2004toward} defined a measurement framework for evaluating the use of Extreme Programming (XP), XP-EF. XP-EF contains context factors to capture internal project-related variables; adherence metrics to capture XP practice use; and outcome measures to capture external project results (e.g. quality). Modeled after XP-EF, we have developed SP-EF, a measurement framework for software development security practice use. We defined a similarly structured set of measures for recording context factors, practice adherence metrics, and outcome measures, related to the use of security practices in software development.
Rudolph and Schwartz~\cite{rudolph2012critical} conducted a systematic literature review, and Morrison et al.~\cite{morrison2014mapping} produced a technical report on security metrics. We chose the context factors and outcome measures aligned with the findings of these surveys.
The Common Vulnerability Scoring System~\cite{mell2006common} (CVSS) is a framework for communicating the characteristics of vulnerabilities in  (information technology (IT). Our focus is on the software development process and product rather than the individual vulnerability, however we adopt the Confidentiality Requirement, Integrity Requirement, and Availability Requirement elements of CVSS as context factors.
A number of organizations have published lists of security practices, including the Building Security in Maturity Model~\cite{mcgraw2013bsimm} (BSIMM), the Microsoft Security Development Lifecycle~\cite{howard2009security} (SDL), the Software Assurance Forum for Excellence in Code~\cite{simpson2013fundamental} (SAFECode), and the Open Web Application Security Project (OWASP) Software Security Assurance Process ~\cite{martinez2014ssap} (SSAP). Organizations and development teams may use one or more of these lists, however the lists do not specify how to go about choosing, implementing, monitoring, and assessing the effects of a set of security practices.
Mockus and Votta~\cite{mockus2000identifying} used keyword matching to distinguish between new code development, corrective, and perfective maintenance activities in version control system commit logs.
Cleland-Huang et al.~\cite{clelandhuang2006detection} applied classification to detect non-functional requirements in requirements documents, beginning with a keyword-based classification approach, refining the classifier to weight the keyword terms by their likelihood in each type of non-functional requirement. They achieved performance measures ranging from 20-100\% recall, and 3-92\% precision. We adapt their techniques by investigate the presence of practice use rather than the presence of non-functional requirements, and apply the techniques to the project version control system data, bug tracking data, and developer mailing lists, rather than to project specifications.
Ernst and Mylopoulos~\cite{ernst10refsq} applied keyword-based classification to evaluate the evolution of references to quality requirements over the life cycles of eight open-source projects. They did not find a systematic way in which references to quality requirements varied between the projects, and suggested that more refined techniques, e.g. domain-specific taxonomies, might be required to accurately track how requirements evolve in software projects. Our mining process parallels their `signifier extraction' process, however our search terms are drawn from SP-EF rather than generic requirements sources. They termed each occurrence of a requirement reference an `event', a convention we adopt in our work.
Hindle [29], building on Cleland-Huang et al.~\cite{clelandhuang2006detection} and Ernst and Mylopoulos~\cite{ernst10refsq} applied bag-of-words and topic modeling text-mining approaches to extracting software process events from project history data in project documentation, repositories, bug trackers, and mailing lists for FreeBSD and SQLLite, concluding that development practice use could be recovered from project history, although practices differ in how much evidence is available, and in the frequency of their occurrence. In following work, Hindle [30] used lists of quality keywords to categorize requirements topics in commit comments from MySQL and MaxDB repositories, using the data to visualize how attention to various requiments changes over the course of the project. Performance for topic recognition ranged between .07 and .93 Recall, and .09 and 1.0 Precision. 
Gegick et al. [31] and Williams et al. [32] have put forward lists of keywords related to security. We adopt these terms in our searches to identify potentially security-related issues, emails, and commits that are not marked by the project as security-related and that are not identified by our practice-specific keywords.
Bayesian Belief Networks share many similarities with SEM models, and have been explored in multiple ways in the software engineering literature. Fenton 

