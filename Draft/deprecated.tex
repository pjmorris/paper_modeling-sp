
For comparison, we ran a linear regression on logCVECount and all of the modeled measurement variables, results shown in Table \ref{tab:nvd_initial_regression}.

%stargazer(lm(logCVECount ~ cvss_access_complexity + adherence + cvss_access_vector + %cvss_auth + cvss_conf_impact + cvss_avail_impact + %cvss_integ_impact,data=rtrunc),style="asr")
% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Fri, Mar 10, 2017 - 13:56:08
\begin{table}[!htbp] \centering 
	\caption{lm(logCVECount ~ cvss\_access\_complexity + adherence + cvss\_access\_vector + cvss\_auth + cvss\_conf\_impact + cvss\_avail\_impact + cvss\_integ\_impact)} 
	\label{tab:nvd_initial_regression} 
	\begin{tabular}{@{\extracolsep{5pt}}lc} 
		\\[-1.8ex]\hline \\[-1.8ex] 
		\\[-1.8ex] & logCVECount \\ 
		\hline \\[-1.8ex] 
		cvss\_access\_complexity & $-$0.038$^{**}$ \\ 
		adherence & 0.038$^{***}$ \\ 
		cvss\_access\_vector & $-$0.083$^{***}$ \\ 
		cvss\_auth & $-$0.172$^{***}$ \\ 
		cvss\_conf\_impact & 0.027 \\ 
		cvss\_avail\_impact & 0.115$^{***}$ \\ 
		cvss\_integ\_impact & $-$0.114$^{***}$ \\ 
		Constant & 2.075$^{***}$ \\ 
		\textit{N} & 10,621 \\ 
		R$^{2}$ & 0.030 \\ 
		Adjusted R$^{2}$ & 0.030 \\ 
		Residual Std. Error & 0.525 (df = 10613) \\ 
		F Statistic & 47.378$^{***}$ (df = 7; 10613) \\ 
		\hline \\[-1.8ex] 
		\multicolumn{2}{l}{$^{*}$p $<$ .05; $^{**}$p $<$ .01; $^{***}$p $<$ .001} \\ 
	\end{tabular} 
\end{table}


For comparison, we ran a linear regression on logCVECount and all of the modeled measurement variables, results shown in Table \ref{tab:cii_initial_regression}.
%stargazer(lm(logCVECount ~ total_contributor_count + ContributorRisk + total_code_lines + LanguageRisk + direct_network_exposure + process_network_data + potential_privilege_escalation + CodeAge + package_popularity + TeamActivity + CodeComments  + twelve_month_contributor_count + WebsiteRisk,data=ciicooked),style="asr")

% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Wed, Mar 15, 2017 - 14:04:20
\begin{table}[!htbp] \centering 
	\caption{CII Initial Model Regressed on logCVECount} 
	\label{tab:cii_initial_regression} 
	\begin{tabular}{@{\extracolsep{5pt}}lr} 
		\\[-1.8ex]\hline \\[-1.8ex] 
		\\[-1.8ex] & logCVECount \\ 
		\hline \\[-1.8ex] 
		total\_contributor\_count & 0.0001 \\ 
		ContributorRisk & $-$0.081$^{*}$ \\ 
		total\_code\_lines & 0.000 \\ 
		LanguageRisk & 0.113 \\ 
		direct\_network\_exposure & 1.055$^{***}$ \\ 
		process\_network\_data & 0.797$^{***}$ \\ 
		potential\_privilege\_escalation & 0.469 \\ 
		CodeAge & 0.154 \\ 
		package\_popularity & $-$0.00000 \\ 
		TeamActivity & $-$0.137 \\ 
		CodeComments & 0.198$^{**}$ \\ 
		twelve\_month\_contributor\_count & $-$0.0003 \\ 
		WebsiteRisk & $-$0.253 \\ 
		Constant & $-$0.185 \\ 
		\textit{N} & 343 \\ 
		R$^{2}$ & 0.221 \\ 
		Adjusted R$^{2}$ & 0.191 \\ 
		Residual Std. Error & 1.210 (df = 329) \\ 
		F Statistic & 7.196$^{***}$ (df = 13; 329) \\ 
		\hline \\[-1.8ex] 
		\multicolumn{2}{l}{$^{*}$p $<$ .05; $^{**}$p $<$ .01; $^{***}$p $<$ .001} \\ 
	\end{tabular} 
\end{table}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010520.10010553.10010562</concept_id>
	<concept_desc>Computer systems organization~Embedded systems</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10010520.10010575.10010755</concept_id>
	<concept_desc>Computer systems organization~Redundancy</concept_desc>
	<concept_significance>300</concept_significance>
	</concept>
	<concept>
	<concept_id>10010520.10010553.10010554</concept_id>
	<concept_desc>Computer systems organization~Robotics</concept_desc>
	<concept_significance>100</concept_significance>
	</concept>
	<concept>
	<concept_id>10003033.10003083.10003095</concept_id>
	<concept_desc>Networks~Network reliability</concept_desc>
	<concept_significance>100</concept_significance>
	</concept>
	</ccs2012>  
\end{CCSXML}



Statistician Andrew Gelman once critiqued a study as follows `Their effect size is tiny and their measurement error is huge. My best analogy is that they are trying to use a bathroom scale to weigh a feather - and the feather is resting loosely in the pouch of a kangaroo that is vigorously jumping up and down.' ~\footnote{http://andrewgelman.com/2015/04/21/feather-bathroom-scale-kangaroo/} We suspect that measuring the effects of security practice application in software projects is prone to similar difficulties, given the wide variety of software development processes and software applications. To measure the effect of security practices on vulnerabilities, we have to account for the other influences on vulnerabilities. To do so, we build a model of the variables we expect to influence security outcomes, and to account for the various factors that influence software project performance.


As evidenced by the Heartbleed~\footnote{\url{http://heartbleed.com/}} and Shellshock~\footnote{\url{http://tinyurl.com/mfv93td}} vulnerabilities, a single commit, sometimes even a single line of code, can cause a large-scale security problem.  Consequently, software development teams are investigating the use of security practices in software development.

 When presented with lists of the dozens of software development security practices available (e.g. those listed in the `Building Security in Maturity Model ~\cite{mcgraw2013bsimm} (BSIMM)), the first question industry practitioners typically ask is `What security practices should we  adopt?'  To answer this question requires an understanding of the questioner's security problems, and a theory for how software development security practices affect the software. To support collection of the data needed to answer questions about security practices and their effects empirically, we present a model for how security practice use affects security outcomes in software development.  Ultimately, our goal is to provide evidence-based answers for questions like `What security practices should we adopt?', and `What do we do next?'.
 
 A number of organizations have published lists of security practices, including the Building Security in Maturity Model~\cite{mcgraw2013bsimm} (BSIMM), the Microsoft Security Development Lifecycle~\cite{howard2009security} (SDL), the Software Assurance Forum for Excellence in Code~\cite{simpson2013fundamental} (SAFECode), and the Open Web Application Security Project (OWASP) Software Security Assurance Process ~\cite{martinez2014ssap} (SSAP). Organizations and development teams may use one or more of these lists, however the lists do not specify how to go about choosing, implementing, monitoring, and assessing the effects of a set of security practices.
 Mockus and Votta~\cite{mockus2000identifying} used keyword matching to distinguish between new code development, corrective, and perfective maintenance activities in version control system commit logs.

Typical definitions of risk include measures of the size of the potential threat, and the probability that the threat will occur. For example, the NIST recently defined risk ~\cite{nist2011managing} as: `A measure of the extent to which an entity is threatened by a potential circumstance or event, and typically a function of: (i) the adverse impacts that would arise if the circumstance or event occurs; and (ii) the likelihood of occurrence.' In keeping with typical definitions of risk, we define two sub-constructs, Impact, and Likelihood. Impact represents the value an attacker will find in conducting a successful attack on the assets made available through the software.

Adherence represents the efforts the team takes to prevent and discover vulnerabilities. We adapt an IEEE definition of practice~\cite{ieee1990glossary} `3. a specific type of professional or management activity that contributes to 
the execution of a process and that may employ one or more techniques and tools' to define a software development security practice to be an action a software development team member takes to prevent, identify, or resolve a vulnerability, possibly guided by a tool or reference. From our definition, we define a template for describing security practice use: \textless Role\textgreater (team member)\textless Verb\textgreater (action)\textless Artifact(s) Affected\textgreater, (guided by)\textless Artifact(s) Referenced\textgreater. Our definition and template allows description of the actions team members take (e.g. `Developer ran Findbugs before checking in code', `Manager documented threat model for new design') in the course of adhering to security practices during the project. The Adherence construct is measured in terms of the frequency and prevalence of security practice use by the team, and in terms of the vulnerability source to which the practice is applied. To measure frequency, we count instances of practice use in team documentation and communications (e.g. project documentation, emails, commit messages, bug tracking issues), and normalize the count to the total total team effort in the units counted. For example, if there are 10 references in emails to penetration testing and there are 100 emails, the ratio is 0.1.  To measure prevalence, we compute the ratio of team members who apply the practice to total team members. We measure adherence for each security practice to enable comparison of the relative values of the various security practices to which a team adheres. To measure source, we record the vulnerability source (Specification, Coding, Testing, Operations) to which the practice is applied. We conjecture that the types of communication the team use indicate influence on different vulnerability sources. For example, we conjecture that practice adherence in commit messages primarily represents security effort during Coding,  and that practice adherence in bug tracking issues is more likely to represent security effort in Testing and/or Operations. 

\subsection{Putting it all together; Linking SPEF to the Model}

Figure X-marks-the-full-model-spot presents the model, annotated with each SPEF data element in its hypothesized relationship to the model. 
In this section, we build three graphical models of the hypothesized relationships described above, representing three levels of measurement detail.

\input{CAO_SCTO_SPEF_Table}


We select projects for this study based upon meeting the following criteria:
\begin{itemize}
	
	\item Available records of software security vulnerabilities
	\item Version control system access, providing both project source code and the history of changes to the code over time
	\item Bug tracker system access, providing records of vulnerability and defect discovery and resolution
	\item Project documentation, providing access to information about the project’s development process and practices
\end{itemize}

Consequently, in SEM, a model is posited that specifies the relationships among all variables (latent and observed) resulting in systems of linear equations such that the relationships between all variables are linear (or transformably linear, for example by log or square root transformation). In these linear equations, variables are linked by structural parameters denoted as $\Theta$. Based on these equations, the population covariance matrix of the observed variables, $\Sigma$, can be represented as a function of the model parameters, $\Sigma=\Sigma(\Theta)$, where $\Sigma$ is the population covariance matrix of the observed variables, $\Theta$ is a vector of model parameters,  and $\Sigma(\Theta)$ is the covariance matrix as a function of $\Theta$. 

In SEM,$\Sigma$ is derived from datasets representing a sample drawn from the population, and `estimator' algorithms are applied to the model and dataset to solve for $\Theta$, the model parameter estimates. Hildreth ~\cite{hildreth2013residual}, Chapter 2, gives a clear presentation of the linear algebra involved in specifying and solving SEM models. 

We applied the Boa [44] language infrastructure to the full September 2015 Github dataset ro collect project summaries for 68,035 Java language projects We operationalize practice adherence by counting references to each practice’s keywords in commit messages. 

To describe the Outcomes construct, we begin with a definition of vulnerability. Following Krsul~\cite{krsul1998software} and Ozment~\cite{ozment2007vulnerability}, we define a software vulnerability as “an instance of a mistake in the specification, development, or configuration of software such that its execution can violate the explicit or implicit security policy.  


  Vulnerabilities may not only be problems in code (`bugs'~\cite{mcgraw2006software}) but may also be, for example, design issues (`flaws'), documentation issues, and configuration issues.  
  
  
  Vulnerabilities may be manifest, when discovered by users, security researchers or the development team itself, or they may be latent, not yet discovered by users, researchers or the team. 
  
  Pre-release vulnerabilities are an indication that the development team has incorporated practices supporting discovery of vulnerabilities into its development process. 
  
  Post-release vulnerabilities are an indication of both vulnerabilities that have escaped the development process and of attacker interest in the software.
  
To identify a practice, we adapt Ernst and Mylopoulos’s [28] technique of labeling events in project history, though we measure security practice use rather than requirements discussions, and we use the SP-EF keywords, listed in Table 1, rather than drawing from the ISO 9126 quality model. 

We developed a Boa job  that scans Boa datasets for projects, and produces a single-line summary for each project, including the project name, url, Source Lines of Code (SLOC), number of developers (Devs), references to ‘CVE-‘ records, and keyword counts for each of the security practices and the ‘Security-Related’ keywords.  

To exclude toy projects, we limited the results to those projects with two or more developers, and 1000 or more lines of code.  Because Boa only stores source code information for Java projects, our results include only Java projects. We operationalize security outcomes by counting references to CVE’s in commit messages.  We identified 121 unique projects with one or more CVE’s. 

We randomly sampled 121 projects with no CVE’s from the remaining projects to form a data set of 242 projects.
We were not able to obtain all SP-EF measurements from the repositories. 

To compensate, we kept the structural model intact and adapted or removed some elements of the measurement model to account for the data available from our data sources. 

We now list the adaptations. Since all projects are written in Java, we elided language from the model. 

We removed domain, number of identities, CR, IR, and AR because any selection process we could develop to set values for the variables would add noise to the data. Because of the nature of commit logs, we assume that each unique email address associated with the commit is for a developer. We use the number of developers (‘Devs’), as a proxy for team size. As a proxy for number of machines, we used the number of Google  search results for each repository, reasoning that the search result counts – how widely known the project is - is proportional to how widely used the project is We use counts of CVE references as a proxy for total public vulnerabilities (pre- and post-release), and we calculate VDensity. We do not have a breakdown of when vulnerabilities were discovered for each project, and so we do not measure Pre-Release Vulnerabilities, Post-Release Vulnerabilities, and we do not calculate their ratio, VRE. We use the CVE count and VDensity as our measures of security outcomes. 


\item Estimate number of observations required
Count the edges between each structural model and measurement model variable, multiply by 2 to account for both the variable's parameter estimate and its error term, and by 10 to account for the number of observations required to estimate each parameter. If the dataset is smaller than the number of observations required, consider whether a dataset of sufficent size can be bootstrapped.


\subsubsection{Specification}
For causal hypotheses, we used the hypotheses for each construct in Section \ref{sec:model} to create edges in the model specifying each element’s effect on security risk, practice adherence, and security outcomes.  We model security risk, security effort, and security outcomes as latent variables, explanatory entities that reflect a continuum that is not directly observable~\cite{kline2015principles}. In SEM, latent variables are represented in diagrams as ovals or circles.

\subsubsection{Identification}
In identification, the specified model is checked against statistical theory for whether all of the models parameters can be estimated. Identification is analogous to checking whether a set of equations is solvable given their structure and the data at hand.  Our construct model contains no feedback loops making it both recursive, and, therefore, identified, by Rule 7.1 of Kline ~\cite{kline2015principles}.  



The Common Criteria (CC) ~cite{commoncriteria2012introduction} model for security concepts can be summarized in the following five statements:
\begin{itemize}
	\item Owners value Assets
	\item Owners wish to minimize Risk to Assets
	\item Owners impose Countermeasures to reduce Risk
	\item Threat agents wish to abuse or damage Assets
	\item Threat agents give rise to Threats to Assets that increase Risk
\end{itemize}

We adapt the CC model: we do not model actors, e.g. Owners and Threat agents, and we augment the model with measures of the constructs. Removing actors from consideration allows the model to be evaluated independent of specific accountability and simplifies the model. Risk to Assets, Threats to Assets increase Risk, Countermeasures decrease Risk. Translating to the terms used in our measurement framework, Risk is described in terms of Outcome Measures and Context Factors, Assets are described in terms of Context Factors, and Countermeasures are described in terms of Practice Adherence.

We define a set of vulnerability sources, drawn from our definition:  
\begin{itemize}
	\item \textbf{Specification} - All activities, e.g. planning, preparation, requirements definition, and design that precede, chronologically or conceptually, coding. 
	\item \textbf{Code} - The development and maintenance of software features.
	\item \textbf{Test} - The quality assurance activities applied to developed software before it is released.
	\item \textbf{Operations} - The configuration and use of the released software, and feedback from the software's users to the development organization.
\end{itemize} 

The vulnerability sources may also be viewed chronologically, as development phases. Boehm ~\cite{boehm1981economics} reports that the earlier in the development process a defect is resolved, the cheaper it is to resolve. We can use data collected according to this scheme to analyze whether vulnerabilities follow the same cost pattern as defects.  While many projects will have a more detailed set of development process sources (phases), we conjecture that these phases are present in the majority of software development projects.


For each practice, we used the search strings listed in the ‘Keywords’ column of Table 1, and recorded each instance of practice use.  We collected data by two means:
\begin{itemize}
	\item We used a script, available from the SP-EF website, to iterate over each commit, issue, and email, and generate security practice event classifications.
	\item The first author manually examined each project for security practice use and generated security practice event classifications. Additional raters classified a randomly selected pool of issues, and we compared their results to the automated classifications.
\end{itemize}

For each artifact we identified, we recorded the document name, URL, age, and made note of any available change history, e.g. wiki page change records. We manually classified pre- and post-release vulnerabilities based on a study of who reported the vulnerability and whether they were identifiable as a project member.

Don't forget about the case_* input files

\input{case_openssl}

\begin{table*}
	\begin{center}	
		\caption{Case Study Context Factors: OpenSSL and phpMyAdmin}
		\begin{tiny}	
			\begin{tabular}{|l|l|l||l|l|}
				%\topline
				%\headcol & \multicolumn{2}{c}{OpenSSL} & \multicolumn{2}{c}{phpMyAdmin}\\
				%\headcol & Before & After & Before & After \\	
				%\midline
				Churn & & & & \\
				Confidentiality Requirement & High & High & High  & High \\
				Integrity Requirement & High & High & High  & High \\
				Availability Requirement & High & High & Low  & Low \\
				Dependencies & glibc & glibc & OS, Web Server, MySQL, Browser &  OS, Web Server, MySQL, Browse \\
				Domain & networking, security & networking, security & Database administration & Database administration \\
				Number of Identities & Millions & Millions & Millions & Millions \\
				Language & C & C & PHP, Javascript, HTML SQL & PHP, Javascript, HTML SQL \\
				Number of Machines & 900,000 & 3,700,000 & Millions & Millions \\
				Methodology & & & BDFL & BDFL \\
				Operating System & Unix, Windows, OS X, Open VMS & Unix, Windows, OS X, Open VMS & Unix, Unix \\
				Product Age & 18 years & 18 years & 18 years & 18 years \\
				SLOC & 456,000 & 438,000 & 154,000 & 389,000 \\
				Source Code Availability &  Open Source &  Open Source &  Open Source &  Open Source \\
				Team Location & Distributed  & Distributed  & Distributed  & Distributed \\
				Team Size (Devs,Testers) & 15  & 15 & 9 & 9 \\
				%\bottomline
			\end{tabular}
			
			\label{tab:CAO_SCTO_Table}
		\end{tiny}
	\end{center}
\end{table*}

\begin{table*}
	\begin{center}	
		\caption{phpMyAdmin Practice Adherence Comparison Table}
		\begin{tiny}
			
			\begin{tabular}{|l|rrr||rrr|}
				%\topline
				%\headcol & \multicolumn{3}{c}{Before} & \multicolumn{3}{c}{After}\\
				%\headcol  & Observed & Oracle & Mined  & Observed & Oracle & Mined \\ 
				%\headcol  SP-EF Security Practice  & Frequency & Count & Count & Frequency & Count & Count\\
				%\midline	
				Apply Data Classification Scheme & &  &  & N/A & & 2\\
				Apply Security Requirements  & &  & 25  & Weekly & 6 & 50\\
				Perform Threat Modeling &  &  & 2  & Weekly & & 9\\
				Document Technical Stack  &  &  & 123  & Monthly & & 496\\
				Apply Secure Coding Standards  &  &  & 90  & Daily & & 754\\
				Apply Security Tooling  &  &  & 19 & Daily & & 129\\
				Perform Security Testing &  &  & 48  & Weekly & 285 & 521\\
				Perform Penetration Testing  &  &  & 3  & Monthly & & 0\\
				Perform Security Review  &  &  & 2  & Daily & 7 & 40\\
				Publish Operations Guide &  &  & 101 & Monthly & & 388\\
				Track Vulnerabilities & & 77 & 17  & Weekly & 166 & 498\\
				Improve Development Process & & 44 &  & Monthly & 48 & 4\\
				Perform Security Training &  &  & 25 & N/A & & 196\\
				%\midline
				CVE & 17 & N/A & N/A & 28 & N/A & N/A\\
				Security-Related &  &  & 45 & & & 132 \\
				
				%\bottomline
			\end{tabular}
			
			\label{tab:openssl_paComparisonTable}
		\end{tiny}
	\end{center}
\end{table*}


\begin{table*}
	\begin{center}	
		\caption{OpenSSL Practice Adherence Comparison Table}
		\begin{tiny}
			
			\begin{tabular}{|l|rrr||rrr|}
				%\topline
				%\headcol & \multicolumn{3}{c}{Before} & \multicolumn{3}{c}{After}\\
				%\headcol  & Observed & Oracle & Mined  & Observed & Oracle & Mined \\ 
				%\headcol SP-EF Security Practice  & Frequency & Count & Count & Frequency & Count & Count\\
				%\midline	
				Apply Data Classification Scheme & &  & 0 & N/A & & 2\\
				Apply Security Requirements  & &  & 13 & Weekly & & 67\\
				Perform Threat Modeling &  &  &  & Weekly & & 18\\
				Document Technical Stack  &  & 2 & 114 & Monthly & 6 & 433\\
				Apply Secure Coding Standards  &  &  & 282 & Daily & & 553\\
				Apply Security Tooling  &  & 0 & 12 & Daily & 0 & 65\\
				Perform Security Testing &  & 0  & 212 & Weekly & & 261\\
				Perform Penetration Testing  &  & & 0 & Monthly & & 0\\
				Perform Security Review  & Daily & 0 & 12 & Daily & 2 & 543\\
				Publish Operations Guide &  &  & 81 & Monthly & & 215\\
				Track Vulnerabilities & & 5 & 181 & Weekly & 99 & 197\\
				Improve Development Process & & 0 & 0 & Monthly & 4 & 1\\
				Perform Security Training &  &  & 56 & N/A & 12 & 36\\
				%\midline
				CVE &  &  &  & &  & \\
				Security-Related &  &  & 20 & & & 242 \\
				
				%\bottomline
			\end{tabular}
			
			\label{tab:pma_paComparisonTable}
		\end{tiny}
	\end{center}
\end{table*}

\input{case_pma}

