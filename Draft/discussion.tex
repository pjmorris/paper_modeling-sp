\section{Discussion}
\label{sec:discussion}

In this section, we first discuss each case study. We then compare our expectations and findings across our case study results. Finally, we present implications from our findings for our method, model, and data. 

\subsection{NVD Case Study}
\label{sec:case_nvd_discussion}
In this section, we discuss how the measured metrics aligned with the structural model constructs for the NVD data.

\subsubsection{Asset Impact}
Asset Impact is correlated most strongly with the Confidentiality Impact measurement (.88 standardized), however the Integrity Impact (.76) and Availability Impact (.70) are in the same range and direction.
We found that Asset Impact's effect (0.05 standardized) on Outcomes was roughly 40\% that of Software Risk's effect (0.12 standardized.)

\subsubsection{Software Risk}
In terms of the measurements we used, more recent years are associated with higher Software Risk. Software Risk is affected most strongly by CVSS Authentication (0.38 standardized). Whether authentication has been implemented appears to be a significant factor, relative to the other CVSS measures. We conjecture that high CVSS Authentication values reflects positively on development team efforts to secure their software through increasing use of authentication. CVSS Access Vector metrics have been increasing over the last 15 years.  We conjecture that the increase in web usage over the last 15 years correlates with the increase in network attacks represented by increasing CVSS Access Vector values.

We have to explain the unexpected negative association of CVSS Access Complexity with Software Risk. One possible explanation is that Access Complexity is a poor measurement. The maintainers of the CVSS standard have acknowledged the overloaded nature of Access Complexity. They state that it is impossible to tell whether a reported instance of Access Complexity is due to user interaction from social engineering, a race condition, incorrect configuration, or some other factor~\footnote{https://www.first.org/cvss}. Version 3 of the CVSS (CVSSv3) scoring system includes a separate `User Interaction' metric to disambiguate between user-driven and software-driven reasons. In future work, we can update the measurement model to account for the updated CVSSv3 metrics. A second possible explanation is that the measurement model is incorrect, due to the Access Complexity metric being associated with the wrong construct, or due to a broader problem with assignments of metrics to constructs. To test this notion, we fit the model with Access Complexity associated with Asset Impact rather than Software Risk, but the model fit was worse than the initial model. A third explanation is that our structural model is incorrect, either in the relationships between the constructs, or in missing one or more constructs with which Access Complexity could be associated. We put altering the structural model out of scope for the present work, but will investigate refinements of the structural model in future work. 

\subsubsection{Adherence}
In our data, Adherence positively covaries with Software Risk (0.78 standardized). Our modeling did not confirm Kaminsky's hypothesis that software quality is improving over time. However, we observed an improvement in CVSS $Authentication Risk_{Software Risk}$ (Figure \ref{fig:nvd_vulns_auth}). On average, vulnerabilities are requiring more authentications over time. We conjecture that this implies that development teams are putting an increasing amount of functionality behind access control.

\subsubsection{Outcomes}
Outcomes are only partially explained by the combined effects of Asset Impact (0.05) and Software Risk (0.012). We conjecture that one reason for the small (less than 15\%) effects of Asset Impact and Software Risk on Outcomes are due to the underlying measurement variables being incomplete accounts of the constructs they measure. For example, the $CVSS impact metrics_{Asset Impact}$ say something about potential risks, but do not say anything about the environments in which the software runs, or about the kind of data maintained by the software in the contexts from which vulnerabilities were reported. Similarly, the software risk constructs represented by $CVSS Access Vector_{Software Risk}$, and $Authentication_{Software Risk}$ do not contain information on software size, churn, language, or the other constructs that have been identified as contributing to security issues in software.   

Per the descriptions of the metrics in Section \ref{sec:evaluation_nvd_selection_risk}, we view each of the metrics as measuring a property of the software that the development team can influence. Teams might increase CVSS Access Complexity and CVSS Authentication through the addition of access control mechanisms. Teams might reduce CVSS Access Vector values through care taken with validation of network inputs, and through greater care taken with implementation of network code. Teams might reduce CVSS Authentication values through stricter access controls. Changes over time in each of the Access metrics may be due to development team efforts or to attacker efforts, and we do not have the data here to distinguish between the two cases.  

However, CVSS Access Vector values hve decreased in parallel with the increase in CVSS Authentication, indicating that vulnerabilities are increasingly prone to remote attacks from across the network.

\subsection{CII Case Study}
\label{sec:case_cii_discussion}
In this section, we discuss how the measured metrics aligned with the structural model constructs for the CII data.

\subsubsection{Asset Impact} 
Asset Impact, as measured by the package popularity metric, has a correlation opposite of what we theorized. Our sole measure for Asset Impact in this measurement model is package popularity. If we assume that both the structural and the measurement model are correct, we conjecture that more popular packages may receive more quality control effort. The result bears further investigation.

\subsubsection{Software Risk}
Software Risk is affected most strongly by Contributor Count (.91 standardized), then by SLOC (.59 standardized), consistent with prior research correlating software quality attributes with vulnerabilities, e.g. Shin et al. ~\cite{shin2011evaluating}, Dashevskyi et al. ~\cite{dashevskyi2016on}.

Parallel to the NVD data, the Software Risk constructs represented in the CII data by SLOC and direct network exposure do not contain information on software churn, language, or the other constructs that have been identified as contributing to security issues in software. We dropped the existing CII measure of Language Risk from the model during re-specification, prompting the need for further investigation of alternative measures of the effect of language on Software Risk, as well as further investigation of the structural and measurement models. 

\subsubsection{Adherence} 
In our data, Adherence, as measured by Team Activity, Code Comments, and Website Points, has the theorized negative effect on Software Risk, however the effect is not statistically significant. Given that the metrics we chose are a partial accounting of adherence effort, collecting the complete set of theorized adherence metrics is necessary to investigate the Adherence - Software Risk relationship.

\subsubsection{Outcomes}
Outcomes are not completely explained by the combined effects of Asset Impact (-0.14 standardized) and Software Risk (0.28 standardized). We found small effects for both Asset Impact and Software Risk on Security Outcomes, with Software Risk having an effect (.28 standardized) roughly twice the magnitude of Asset Impact (-0.14). We expect that the small (less than 28\%) effects of Asset Impact and Software Risk are due to the underlying measurement variables being incomplete accounts of the constructs they measure.

Per the descriptions of the metrics in Section \ref{sec:cii_selection}, we view each of the metrics as measuring a property of the software that the development team can influence.
Team Activity can be altered through management decisions about allocation of budget and personnel to software development and security assurance tasks. Code Comments and Website Points may be direct measures, or indirect measures, of effort and thought put in to documentation, which correlates in our data with lower Software Risk, consistent with the CII teams's hypothesis.

\subsection{Case Study Comparison}
%	actual(expected)magnitude*statsig
%		== signs 'as hypothesized'
%		
%			NVD				CII
%	AR-SO	+(+)0.05*		-(+)0.14*
%	SR-SO	+(+)0.12*		+(+)0.28*
%	PA-SR	+(-)0.78*		-(-)0.44
Asset Impact was statistically significant in both the NVD and CII datasets. The 0.14 magnitude in the CII dataset was roughly triple the 0.05 magnitude in the NVD dataset. We attribute the magnitude difference primarily to the different metrics used to assess Software Risk in each dataset. Asset Impact was proportionally smaller than Software Risk for both datasets. The data available in the NVD and CII datasets are a subset of the metrics we expect to influence Asset Impact. We conjecture that more accurate asset data would affect the proportion of Asset Impact. In the case of CII Asset Impact's negative association with Outcomes, we observe that the sole measure of CII Asset Impact is package popularity, an indicator within the Debian ecosystem of the number of systems with that package installed. While in the present case, running on more machines is associated with fewer vulnerabilities, contrary to our hypothesis, the effect may be an illustration of Eric Raymond's 'many eyes make all bugs shallow', as measured by the number of eyeballs of users of the software. A number of studies have examined Linus' law in the context of developer attention (e.g. ~\cite{meneely2013when}) rather than user attention, as measured here. 
Software Risk, as hypothesized, positively affected Outcomes, and was statistically significant, in both the NVD and CII datasets. The 0.28 magnitude in the CII dataset was roughly double the 0.12 magnitude in the NVD dataset. We attribute the magnitude difference primarily to the different metrics used to assess Software Risk in each dataset. We conjecture that the metrics available in each dataset are of value for assessing Software Risk.  Of particular note are the network risk metrics each dataset provides. Both NVD cvss\_access\_vector and the similar CII direct\_network\_exposure metrics were statistically significant contributors to Software Risk. Larger projects, as measured by team size and code size, are more likely to have CVE's reported against them. While not a new finding, our data and model are in agreement with prior research~\cite{shin2011evaluating,meneely2013when}.

Adherence positively (NVD, contrary to hypothesis) and negatively (CII, as hypothesized) affected Software Risk, and was statistically significant in the NVD dataset. The 0.78 magnitude in the NVD dataset was roughly double the magnitude in the CII dataset, again something we attribute to the different metrics used to assess Adherence in each dataset. Our NVD data did not confirm Kaminsky's hypothesis that software quality is improving over time, with our adherence-over-time measure increasing Software Risk. Our CII data did not contain a similar metric. However, we observed several counter-trends in the underlying NVD data. We observed an improvement in CVSS Authentication Risk (Figure \ref{fig:nvd_cvss_auth}). On average, vulnerabilities are requiring more authentications over time, implying that development teams are putting an increasing amount of functionality behind access control. One implication of our study is the  need to collect and evaluate a set of adherence metrics for assessing Adherence and its effects on mitigating Software Risk.

The datasets we studied did not contain a complete set of the measurement model metrics we are interested in, as listed in Table \ref{tab:model_spef_metrics}. For the metrics the datasets did contain, we found statistically significant support for each of them, with the exceptions of process\_network\_data, potential\_privilege\_escalation, Code Comments, and Website Risk from CII. 

For the structural model, while our current evidence for the constructs and their relationships is equivocal, we have a set of conjectures and the data they require to establish whether we can retain the model. 

We view our contributions in this work to be the following:
\begin{itemize}
	\item A proposed model of the constructs affecting software development security outcomes
	\item A proposed set of metrics to measure for assessing security posture in software development
	\item Empirical evaluation of the proposed model and metrics using two open source datasets
	\item A process for revising the model and metrics, supported by SEM for statistical analysis of model and metric fit. 
\end{itemize}	

Stepping back from the details of the case study measurements, we can use the current model, its estimates, and its residuals, as a baseline for where we stand in terms of measuring the effects of multiple constructs on security outcomes in software development. In particular, we can use the model as 'straw man' for evaluating alternative constructs affecting software security outcomes, we can use the parameter estimates as to assess the relative effects of the constructs we measure, and we can use the residuals as a guide to improving measurement of the constructs we choose. 

We opened the paper with the example of Cloudbleed, illustrating the need to assess software context as well as software quality. We have built a modeling process, identified metrics to collect, collected data, and analyzed the data, supporting the notion that Asset Impact is a component of security Outcomes, in addition to Software Risk. Further development of the model and its constructs, and the metrics and their measurement and collection should provide further insight into the software development and usage constructs affecting security outcomes for the software's developers, managers, and users.