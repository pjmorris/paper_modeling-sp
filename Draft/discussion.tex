\section{Discussion}
\label{sec:discussion}
\label{sec:case_openhub_discussion} 
 
In this section, we first compare our expectations with our findings for our case study results. We then present implications from our findings for our method, model, and data.

In our data, vulnerability counts are as influenced by the number of people using the software as they are by  traditional software development risk metrics. We found effects for both Usage Risk and Development Risk on Security Outcomes, with Usage Risk having an effect (0.34 standardized) comparable to that of Development Risk (0.40 standardized). As a check on this finding, we ran two linear regressions on CVECount, using a) all the other variables, and b) all the other variables without user\_count. Adjusted $R^2$ without user\_count was 0.26, Adjusted $R^2$ with user\_count was 0.37. Our data and analysis suggest that usage is a significant factor associated with publicly reported vulnerabilities, as predicted by, e.g., Zimmermann et al.~\cite{zimmerman2010searching}. Software usage must be taken in to consideration when evaluating software security.

In our data, lower rates of source code change are associated with lower vulnerability counts. In our data, Adherence has a negative effect on Development Risk ($-0.58$ standardized), however the effect was not statistically significant (p-value = 0.17). Our proxy measure for adherence, Developer Attention, the ratio of developers to lines of code, showed a negative effect on Development Risk. We conjecture that high levels of software process effort, including security practices, will take effort not otherwise required during software development, and so will affect developer productivity. We acknowledge the present measure as a placeholder, and recognize the need for further measures of security practice adherence. We have begun developing measures (e.g. in ~\cite{morrison2017surveying}), and intend to evaluate and incorporate these adherence measures in future work.  

In our data, the Development Risk metrics most influential on vulnerability counts, by rank, were number of developers, code churn, and SLOC. Development Risk is correlated most strongly with twelve\_month\_contributor\_count (0.88), followed by code\_churn (0.77), total\_code\_linest (0.46), and project\_age (0.11, not statistically significant, p-value=0.326).  Given the single measurements of each construct, we have no information on the relative importance of the measurements for the Usage Risk, Adherence, and Outcomes constructs.

Our measurement model metrics are correlated with vulnerability counts. The datasets we studied did not contain a complete set of the measurement model metrics we are interested in, as listed in Table \ref{tab:model_spef_metrics}. For the metrics the datasets did contain, we found statistically significant support for each of them, with the exception of project\_age. 

Our case study model and metrics only partially explain software development project vulnerability counts.  We expect that at least one reason for the small combined effects of Usage Risk and Development Risk on Outcomes (Respecified SEM model, $R^2$ 0.38) is due to the underlying measurement variables being incomplete accounts of the constructs they measure. In the present example, only the number of users is considered, not the theorized effects of, e.g. number of machines, number of dollars affected, or the presence of sensitive data. Similarly, the development risk measurements do not include, at present, language, operating system, domain, or the other variables that have been identified as contributing to security issues in software. Measuring the $R^2$ of our constructs and measurements correlation with Outcomes gives us an assessment of the efficacy of our model and framework, a means for measuring the effect of changes to the structural and measurement models.

Stepping back from the details of the case study measurements, we propose three benefits of the model we have presented and evaluated: 
\begin{itemize}
\item The structural model constructs provide guidance for the kind of measurements to collect when evaluating software security.
\item SEM fit indicators and parameter estimates provide numerical assessment of model and metric performance, enabling data-driven assessment and iterative improvement of metric use.
\item Because the structural model does not depend on a particular set of measurements, it can be applied at other granularities than the software development project example used in this paper. In future work, we intend to apply the model to evaluate binaries, source files, and commits.
\end{itemize}

We opened the paper with the example of Cloudbleed, illustrating the need to assess software context as well as software quality. We have built a model, identified metrics to collect, collected data, and analyzed the data, supporting the notion that Usage Risk and Development Risk both  correlate with security Outcomes. Further development of the structural model and its constructs, and the measurement model metrics and their measurement and collection should provide further insight into the software development and usage constructs affecting security outcomes for the software's developers, managers, and users.
