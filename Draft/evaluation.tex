\section{Evaluation}
\label{sec:evaluation}

This section describes how we evaluated our model through two case studies of existing software development security data.

We presented the data elements to be collected for our full model in Section ~\ref{sec:model_measurement}, and the data collection guide for the measurement model gives instructions on how to collect the data for a software development project~\cite{morrison2016spefsite}.  SEM is a large-sample technique, with median sample size in the literature of 200 cases~\cite{kline2015principles}. The need for large quantities of software development security data leads us to examine existing software development security datasets. In addition to the practical considerations of requiring large amounts of data, confirmation of our hypothesized structural and measurement relationships in data we did not author would strengthen the case for the theorized relationships.
We have identified two candidate data sets, of increasing detail and decreasing observation count: 
\begin{itemize}
\item The National Vulnerability Database contains vulnerability records for a wide variety of software, over a long timespan.
\item The Core Infrastructure Initiative project census contains high-level project data for over 400 Debian~\footnote{https://www.debian.org/} packages.
\end{itemize}

We give further details for each of these datasets in their case study sections, below. We used the R~\footnote{https://www.r-project.org}  lavaan~\cite{roseel2012lavaan} (LAtent VAriable ANalysis) package to conduct our SEM analysis, as well as the ggplot2, semPlot and psych R packages.

\subsection{National Vulnerability Database Case Study}

The U.S. National Institute of Standards and Technology (NIST) maintains the National Vulnerability Database (NVD) ~\footnote{https://nvd.nist.gov/}, an online database of publicly reported software vulnerabilities, with over 79,000 vulnerabilities dating back to 1988. Vulnerability reporters assign each vulnerability a Common Vulnerability Scoring System (CVSS) score and associated CVSS base metrics, according to the scheme defined in the CVSS guide~\cite{mell2007complete}.  

\subsubsection{Data selection}
 We translate the CVSS metrics into the terms of our model constructs and measurements. For each structural model construct, we present our metric associations and the rationale behind them.
 
\textbf{Asset Value}
The CVSS Confidentiality Impact, Integrity Impact, and Availability Impact metrics measure the degree of loss of confidentiality, integrity, and availability represented by a reported vulnerability~\cite{mell2007complete}. The CVSS Impact values:
	\begin{itemize}
		\item 'None' indicates no impact, 
		\item 'Partial' indicates partial impact, and 
		\item 'Complete' indicates 'Complete' impact to the CIA property of the system by the exploited vulnerability.  
	\end{itemize}
We model each Impact metric as a component of Asset Value, theorizing that confidentiality, integrity, and availability impacts change based on the usage context in which the software is run, e.g. the value of the assets managed. We translate None/Partial/Complete to an ordinal scale to model increasing CIA impact risk.

\textbf{Software Risk}

The CVSS Access Vector, Access Complexity, and Authentication metrics capture how a vulnerability is accessed and whether or not extra conditions are required to exploit it.~\cite{mell2007complete}. We translate these metrics into the terms of our model constructs and measurements. For each metric, we now quote the CVSS guide definition, and give a rationale for the associations we have defined:
\begin{itemize}
	\item Access Vector - measures how the vulnerability is exploited, in terms of network distance. Values:
	\begin{itemize}
		\item 'Local' requires the attacker to have physical access or an account on the system, \item 'Adjacent Network' requires access to the physical network on which the vulnerable software resides, and \item 'Network' requires only logical network acccess, e.g. remote access. 
	\end{itemize}
	We model Access Vector as a component of Software Risk, theorizing that the vector value changes based on design choices made by the software development team. We translate Local/Adjacent Network/Network to an ordinal scale, 1, 2, 3 to model increasing `Access Vector Risk'.
	\item Access Complexity - measures the complexity of attack required to exploit a vulnerability once an attacker has gained access. Values: 
	\begin{itemize}
		\item 'High' indicates specialized/elevated access is required to exploit the vulnerability, 
		\item 'Medium' indicates that some access is required to exploit the vulnerability, and \item 'Low' indicates that the vulnerability can be exploited with default/no special access.  
	\end{itemize}
	We model Access Complexity as a component of Software Risk, theorizing that the complexity value changes based on design choices made by the software development team. We translate High/Medium/Low to an ordinal scale, 1, 2, 3 to model increasing `Access Complexity Risk'.
	\item Authentication - measures the number of times an attacker must authenticate to exploit a vulnerability. Values: 
	\begin{itemize}
		\item 'Multiple' indicates an attacker must authenticate two or more times to exploit the vulnerability, 
		\item 'Single' indicates that an attacker must authenticate once to exploit the vulnerability, and 
		\item 'None' indicates that the vulnerability can be exploited without authenticating.  
	\end{itemize}
	We model Authentication as a component of Software Risk, theorizing that the authentication requirement changes based on design choices made by the software development team. We translate Multiple/Single/None to an ordinal scale, 1, 2, 3 to model increasing `Authentication Risk'.
\end{itemize}
  
\textbf{Adherence}
We model Adherence via the passage of time; we take the year each vulnerability is reported as indicative of the state of security practice adherence at the time the vulnerability was reported. We define an `adherence' metric for each vulnerability as the difference, in years, between the vulnerability's publication date and the initial year in the NVD database. If software quality (presumed to be caused by practice adherence) is increasing, we would expect to see a positive correlation between year and the CVSS impact metrics.  In an earlier test of this hypothesis, Kaminsky et al. ~\cite{kaminsky2011showing} fuzzed ten years of office software releases, confirming that software quality had improved over the previous decade, as measured by crash counts per software release over time. 
    
\textbf{Outcomes}
We obtain a metric for Outcomes by treating each unique software name in the NVD records as a distinct project, and summing all vulnerabilities for a project, reflecting our theorized post-release vulnerability count metric. We group each project by its (software) name, and by publication year, taking the mean of the remaining CVSS metrics to represent that project/publication year. 

% plot vuln count over time
% plot cvss score average
% plot three impact metrics
% plot three access metrics

	
\subsubsection{Data collection}
We collected the entire NVD dataset as of February 2017, but limit our analysis to complete years, from the year 2001 to 2016. The dataset includes vulnerabilities dating to 1988, but years previous to 2001 had relatively low activity (we measured activity for a year as the ratio of vulnerabilities reported that year to the average vulnerabilities per year over the NVD history. Activity first exceeded half of the overall average in 2001.) The years used, with vulnerability counts per year, are listed in Table ~\ref{nvd_vulns_year}.

\begin{table}
	\begin{center}	
		\caption{NVD Vulnerabilities, by year}
		\begin{large}	
			\begin{tabular}{l|l||l|l}
				$2001$ & 1672 & $2009$ & 5731\\
				$2002$ & 2155 & $2010$ & 4639\\
				$2003$ & 1524 & $2011$ & 4150\\
				$2004$ & 2445 & $2012$ & 5289\\
				$2005$ & 4927 & $2013$ & 4856\\
				$2006$ & 6608 & $2014$ & 7546\\
				$2007$ & 6516 & $2015$ & 6415\\
				$2008$ & 5632 & $2016$ & 6430\\
				%\bottomline
			\end{tabular}
			\label{tab:nvd_vulns_year}
		\end{large}
	\end{center}
\end{table}

% stargazer(rtrunc)
% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Sun, Feb 26, 2017 - 22:57:53
\begin{table}[!htbp] \centering 
	\caption{NVD Project Demographics} 
	\label{tab:nvd_demog} 
	\begin{small}
	\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
		\\[-1.8ex]\hline 
		\hline \\[-1.8ex] 
		Statistic & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Max} \\ 
		\hline \\[-1.8ex] 
		CVECount & 10,622 & 4.311 & 5.241 & 2 & 49 \\ 
		logCVECount & 10,622 & 1.466 & 0.533 & 1.099 & 3.912 \\ 
		cvss\_score & 10,621 & 6.139 & 1.429 & 1.200 & 10.000 \\ 
		adherence & 10,621 & 4.678 & 1.007 & 0.225 & 6.532 \\ 
		cvss\_auth & 10,622 & 2.905 & 0.224 & 0.000 & 3.000 \\ 
		cvss\_access\_vector & 10,622 & 2.794 & 0.486 & 0.000 & 3.000 \\ 
		cvss\_access\_complexity & 10,622 & 2.577 & 0.409 & 0.000 & 3.000 \\ 
		cvss\_conf\_impact & 10,622 & 1.833 & 0.504 & 0.000 & 3.000 \\ 
		cvss\_integ\_impact & 10,622 & 1.902 & 0.467 & 0.000 & 3.000 \\ 
		cvss\_avail\_impact & 10,622 & 1.833 & 0.552 & 0.000 & 3.000 \\ 
		\hline \\[-1.8ex] 
	\end{tabular} 
		\end{small}
	
\end{table} 

\subsubsection{Estimation}

Combining the structural and measurement models we have defined with the CVSS data collected from the NVD database, we have the model definition, expressed in lavaan syntax: 

\begin{align}
	$SoftwareRisk =\sim  cvss\_access\_vector +\\ cvss\_access\_complexity + $cvss\_auth$\\
	Outcomes $=\sim  logCVECount$\\
	$Outcomes \sim SoftwareRisk + Adherence + AssetValue$\\
	$Adherence =\sim adherence$\\
	$SoftwareRisk \sim Adherence$\\
	$AssetValue =\sim cvss\_conf\_impact + cvss\_integ\_impact + $cvss\_avail\_impact$\\	
\end{align}		

\subsubsection{Model Fit and Re-specification}
SEM depends on the variances of the measured variables to be within an order of magnitude of each other. The base adherence and CVE Count metrics had variances two magnitudes larger than the other variables. We scaled adherence, and took the log of CVE Count to bring them within range of the other variables. The variation among the 22,000+ projects with exactly one vulnerability caused numeric problems for the estimation algorithm, and we elected to treat the group as outliers and drop them from consideration. We, further, excluded projects with 50 or more vulnerabilities as outliers (79 projects). After the exclusions, 10621 projects remained in the analyzed dataset. 

\subsubsection{Reporting Results}

Table \ref{tab:results_fit_all} presents the fit measure results for the NVD case study (as well as for the other case studies).

\begin{table*}
	\begin{center}	
		\caption{Global Fit Measures and Results}
			\label{tab:results_fit_all}
			\begin{tabular}{p{3cm}p{1cm}|p{2cm}p{2cm}p{2cm}p{2cm}}
				\\[-1.8ex]\hline 
				\hline \\[-1.8ex] 
				Fit Measure &	Threshold & NVD	& CII	& Combined No Net  & Combined Net \\
				\hline \\[-1.8ex] 				
				Number of observations &  & $10621$ & $42$  & 49 & 19 \\				
				Model chi-square &  & $2759.88$ & $42$ & 413.67 & 159.18\\				
				Model d.f. &  & $17$ & $42$ & 105 & 105\\		
				Model p-value & $\leq 0.01$ & $0.0$ & $42$ & $0.0$ & $0.0$\\
				RMSEA & $\leq 0.10$ &  $0.12$  & $42$ & 0.09 & 0.212 \\
				CFI & $> 0.90$ & $0.83$ & $42$ & 0.889 & 0.616\\
				SRMR & $< 0.08$ & $0.08$ & $42$& 0.12  & 0.178 \\
				\hline \\[-1.8ex] 				
			\end{tabular}
	\end{center}
\end{table*}

We report the estimated parameter values for the NVD structural and measurement models in \ref{tab:results_nvd}.

\begin{table}
	\begin{center}	
		\caption{NVD Initial Model Results}
		\label{tab:results_nvd}
		\begin{tabular}{l|rrrr}
				\\[-1.8ex]\hline 
				\hline \\[-1.8ex] 
			\textit{Latent Variables}:  & & & & \\  
			$\sim$ Measured variables& Estimate & Std.Err & z$-$value & $P(>|z|)$ \\
				\hline \\[-1.8ex]
			$SoftwareRisk =\sim$  & & & & \\                                   
			cvss\_ccss\_vctr   & 1.000 & &  & \\                             
			cvss\_ccss\_cmpl &  $-$2.19 &   0.235 &  $-$89.320 &   0.000\\
			cvss\_auth     &   $-$1.403  &  0.148  & $-$9.49   & 0.000\\
			$AssetValue =\sim$     & & & & \\                                    
			cvss\_conf\_mpct   & 1.000     & & & \\                       
			cvss\_intg\_mpct   & 0.798   & 0.011 & 70.746 &   0.000 \\
			cvss\_aval\_mpct  &  0.872   & 0.013 & 67.489   & 0.000 \\
			$Outcomes =\sim$    & & & & \\                                     
			CVECount     &  1.000  & & & \\                          
			$Adherence =\sim$   & & & & \\                                      
			adherence    &     1.000        & & & \\                    
			Regressions:  & & & & \\  
			%& Estimate & Std.Err & z$-$value & $P(>|z|)$ \\
			$Outcomes \sim$         & & & & \\                                     
			SoftwareRisk   &  73.07 &   39.52 & $-$0.013 &   0.992 \\
			Adherence       &  $-$2.84  &  6.829  &  -0.468  &  0.640\\
			AssetValue     &   0.623  &  0.0124  &  5.023 &   0.00\\
			$SoftwareRisk \sim$        & & & & \\                                  
			Adherence     &    0.046 &   0.005  &  7.579 &   0.000\\
			Covariances:  & & & & \\  
			%& Estimate & Std.Err & z$-$value & $P(>|z|)$ \\
			$AssetValue \sim\sim$          & & & & \\                                 
			Adherence      &  $-$0.073  &  0.010 &  $-$7.456 &   0.000\\
		\end{tabular}
	\end{center}
\end{table}

In terms of global fit, the NVD model was just outside the range of traditional fit criteria, as measured by the fit indexes. In terms of the parameter estimates for our hypothesized  construct relationships, we have the following:
\begin{itemize}
	\item  Asset Value - Security Outcomes is positive (0.623), as hypothesized.
	\item Software Risk - Security Outcomes is positive (73.08), as hypothesized. 
	\item Practice Adherence is slightly postitively associated (0.045) with Software Risk 
\end{itemize}	
	  Only the Asset Risk-Outcome estimate was statistically significant. 

\subsection{CII Case Study}

The Core Infrastructure Initiative was formed by the Linux Foundation to support critical elements of the global information infrastructure by identifying and funding projects in need of assistance~\footnote{https://www.coreinfrastructure.org/faq}. The CII team has developed a census of open source projects in potential need of assistance~\footnote{https://www.coreinfrastructure.org/programs/census-project}, as well as a program for measuring and rewarding compliance with good security practice~\footnote{https://bestpractices.coreinfrastructure.org/}.

\subsubsection{Data selection}
The CII census team has published the rationale behind their census and metrics ~\cite{wheeler2015open}, and has published their data and code on Github~\footnote{https://github.com/linuxfoundation/cii-census}. The 429 census records each contain data for one project deemed both important, and potentially at risk for security concerns, by the CII team according to their criteria.

The project record contains descriptive data, e.g. project name and version, and security-relevant metrics, e.g. lines of code, contributor count, as well as an overall `risk index' based on the CII team's estimation of the importance of the metrics used to compute the index value.  Parallel to our translations of the CVSS metric score text to ordinal values, we translate the CII $fact\_activity$, $fact\_age$, $fact\_comments$, and $fact\_team\_size$ columns from text descriptions to ordinal values. We also parse the risk index components field to extract the ordinal ranks the CII team assigned each project for the risk index components: `Website points', `CVE Risk', `12 month contributors', `Popularity', `Language', and (network) `Exposure'. We now present our mapping of the CII metrics to our constructs.

\textbf{Asset Value}
The CII package\_popularity field contains Debian package popularity contest~\footnote{http://popcon.debian.org/} results, a measure of the frequency of use of each package on each system, aggregated to the relative usage of the package within the Debian ecosystem. We reason that paclage popularity is proportional to our theorized `Number of Machines'  metric, and, so, should be modeled as a component of Asset Value.

\textbf{Software Risk}
We model total\_contributor\_count, Contributor Risk, and total\_code\_lines as components of Software Risk, on the rationale that multiple studies (e.g. ~\cite{,camilo2015do,dashevskyi2016on}) have identified team size and code size as contributors to software vulnerability. The CII metrics direct\_network\_exposure and process\_network\_data, and potential\_privilege\_escalation are similar in concept to the CVSS Access metrics, and we, similarly, model them as components of Software Risk, theorizing that their values change based on design choices made by the software development team. We do not use the CII risk index measures, as they are composites, and we wish to examine the behavior of each of the index components.

\textbf{Adherence}
CII census data related to Adherence includes measurements of recent developer activity, and documentation effort. Developer activity is measured in the CII census data via a count of developers committing within the last year, twelve\_month\_contrib, and in terms of an ordinal categorical variable, fact\_activity, which we label TeamActivity. Documentation effort is measured in the CII census data via ordinal categorical variables in the risk index description for the amount and quality of comments, fact\_comments, which we label CodeComments, and by the presence of a website documenting the project, which we label WebsitePoints.

\textbf{Outcomes}
CII census data related to Outcomes includes a count of CVE vulnerabilities reported against the project since 2010. 

\textbf{Field choice notes}
The CII dataset contains a set of essentially co-linear fields, for example CVE Risk is translation of the CVE count since 2010 for the project into a categorical variable. We preferred counts to categorical values in initial field choices for the model.

\subsubsection{Data collection}

We retrieved the CII census data results file, current as of July 2016, from the project Github repository~\footnote{https://github.com/linuxfoundation/cii-census/blob/master/results.csv}. 

% stargazer(ciiraw)

% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Sun, Feb 26, 2017 - 23:20:47
\begin{table*}[!htbp] \centering 
	\caption{} 
	\label{} 
	\begin{small}
	\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
		\\[-1.8ex]\hline 
		\hline \\[-1.8ex] 
		Statistic & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Max} \\ 
		\hline \\[-1.8ex] 
		CVE\_since\_2010 & 428 & 7.262 & 26.142 & 0 & 422 \\ 
		twelve\_month\_contributor\_count & 346 & 98.965 & 491.693 & 0 & 3,768 \\ 
		total\_contributor\_count & 346 & 428.974 & 1,939.063 & 1 & 14,821 \\ 
		total\_code\_lines & 346 & 1,144,234.000 & 2,907,285.000 & 120 & 18,237,262 \\ 
		package\_popularity & 424 & 128,884.800 & 54,885.200 & 1 & 175,853 \\ 
		direct\_network\_exposure & 428 & 0.178 & 0.383 & 0 & 1 \\ 
		process\_network\_data & 428 & 0.152 & 0.359 & 0 & 1 \\ 
		potential\_privilege\_escalation & 428 & 0.058 & 0.235 & 0 & 1 \\ 
		risk\_index & 428 & 6.624 & 2.501 & 1 & 13 \\ 
		CVE & 428 & 7.262 & 26.142 & 0 & 422 \\ 
		RiskIndex & 428 & 6.624 & 2.501 & 1 & 13 \\ 
		TeamActivity & 428 & 1.509 & 0.988 & 0 & 3 \\ 
		CodeAge & 428 & 3.040 & 1.624 & 0 & 4 \\ 
		CodeComments & 428 & 2.231 & 1.773 & $-$1 & 5 \\ 
		TeamSize & 428 & 2.236 & 2.302 & $-$1 & 5 \\ 
		WebsiteRisk & 428 & 0.131 & 0.338 & 0 & 1 \\ 
		CVERisk & 428 & 1.044 & 1.266 & 0 & 3 \\ 
		ContributorRisk & 428 & 1.722 & 1.981 & 0 & 5 \\ 
		PopularityRisk & 428 & 1.808 & 0.494 & 0 & 2 \\ 
		LanguageRisk & 428 & 1.360 & 0.934 & 0 & 2 \\ 
		ExposureRisk & 428 & 0.558 & 0.777 & 0 & 2 \\ 
		DataRisk & 428 & 0.000 & 0.000 & 0 & 0 \\ 
		\hline \\[-1.8ex] 
	\end{tabular} 
	\end{small}
\end{table*}

\subsubsection{Estimation}

Combining the structural and measurement models we have defined with the metric data collected from the CII census, we have the model definition, expressed in lavaan syntax, in Figure \ref{tab:cii_model}.


\begin{align}
$SoftwareRisk =\sim  total\_contributor\_count + total\_code\_lines +$\\ $direct\_network\_exposure + process\_network\_data +$\\ $potential\_privilege\_escalation + CodeAge + TeamSize + LanguageRisk$ \\
$Outcomes =\sim  CVE_since_2010$ \\
$Outcomes \sim SoftwareRisk + Adherence + AssetValue$ \\
$Adherence =\sim  TeamActivity + CodeComments + WebsiteRisk +$ $twelve\_month\_contributor_count$  \\
$SoftwareRisk \sim  Adherence$ \\
$AssetValue =\sim package\_popularity$ \\
\end{align}


\subsubsection{Model Fit and Re-specification}
 
Table \ref{tab:results_fit_cii} presents the fit measure results for the final models of each case study. The baseline model, including all CII data fields, had poor fit indices (e.g. RMSE 0.237). We applied transformations and dropped fields from the model according to our expected theoretical relationships, but did not achieve significantly better model fit for the combinations we tested.

\subsubsection{Reporting Results}

Table \ref{tab:results_fit_all} presents the fit measure results for the CII case study (as well as for the other case studies). We report the estimated parameter values in \ref{tab:results_cii}.

In terms of global fit, the CII model was well outside the range of traditional fit criteria, as measured by the fit indexes. We did not conduct hypothesis tests for the model. 

\subsection{Combined CII and NVD data}
We observed that a subset of the CII census projects have vulnerabilities recorded in the NVD dataset, offering the opportunity to study the combination of measurement variables from the two datasets.  

\subsubsection{Data selection}
We began with the NVD and CII data, collected as previously described. 

\subsubsection{Data collection}
We intersected the two datasets based on the project name used in each dataset. Intersecting the 428 CII projects with the 10621 NVD projects yielded a set of 69 projects with all data for each project.

\subsubsection{Estimation}
We began with the aggregated fields from both datasets, and removed theoretically equivalent fields, preferring ordinal and ratio fields to categorical fields where we had a choice. For example, we chose the `package\_popularity' count over the `PopularityRisk' categorical variable. We evaluated model fit after each variable removal, addition, and transformation.

\subsubsection{Model Fit and Re-specification}
Through a series of experiments, evaluating model fit for the combined datasets, we obtained a `Combined' model with good global fit characteristics, see the `Combined 1' column in Table \ref{tab:results_fit_all}.

\begin{align}
	$SoftwareRisk =\sim cvss\_access\_vector + cvss\_access\_complexity$\\
	$ + cvss\_auth + total\_code\_lines + TeamSize + LanguageRisk$\\
	$Outcomes =\sim  logCVECount $\\
	$Outcomes ~ SoftwareRisk + Adherence + AssetValue$\\
	$Adherence =\sim adherence + twelve\_month\_contributor\_count +$\\ $CodeComments + TeamActivity $\\
	$SoftwareRisk \sim  Adherence$\\
	$AssetValue =\sim cvss\_conf\_impact + cvss\_integ\_impact +$\\ $cvss\_avail\_impact + package\_popularity$\\
\end{align}

\subsubsection{Reporting Results}
To study how models behave for different projects, we selected a non-model variable, `process\_network\_data', to filter data for the model. We compared the performance of the model on projects that do not process network data against the model's performance in projects that process network data by filtering the data each way and applying the model.

\begin{table}
	\begin{center}	
		\caption{NVD-CII Combined, No Net Data}
		\label{tab:results_combined1}
		\begin{tabular}{l|rrrr}
			\\[-1.8ex]\hline 
			\hline \\[-1.8ex] 
			\textit{Latent Variables}: &  & Combined & No Net & \\  
			$\sim$ Measured variables& Estimate & Std.Err & z$-$value & $P(>|z|)$ \\
			\hline \\[-1.8ex]
			$SoftwareRisk =\sim$  & & & & \\                                   
			cvss\_ccss\_vctr   & 1.000 & &  & \\                             
			cvss\_ccss\_cmpl &  $-$0.36 &   0.286 & 1.243 &   0.214\\
			cvss\_auth     &   $-$0.065  &  0.056  & $-$1.17   & 0.244\\
			total\_code\_lns  &  6.232 &   2.572 &   2.423 &   0.015\\
			TeamSize        &  6.268   & 2.484   & 2.524   & 0.012\\
			LanguageRisk    &  0.148  &  0.413   & 0.357   & 0.721\\ 
			& & & & \\  
			$AssetValue =\sim$     & & & & \\                                    		
		    cvss\_conf\_mpct  &  1.000  &	&	&                  \\
		    cvss\_intg\_mpct  &  0.954  &  0.116 &   8.230  &  0.000\\
		    cvss\_aval\_mpct  &  0.896 &   0.122 &   7.361 &   0.000\\
		    package\_pplrty  &  0.458 &   0.464  &  0.987 &   0.324\\	
			& & & & \\  
			$Outcomes =\sim$    & & & & \\                                     
			logCVECount     &  1.000  & & & \\                          
			& & & & \\  
			$Adherence =\sim$   & & & & \\                                      
			adherence    &     1.000        & & & \\    
			
		    twlv\_mnth\_cnt\_  &  7.783  &  4.088  &  1.904 &   0.057\\
		    CodeComments    &  1.914  &  1.145  &  1.672  &  0.095\\
		    TeamActivity    &  1.386  &  0.842 &   1.646 &   0.100\\	
			                
			Regressions:  & & & & \\  
			%& Estimate & Std.Err & z$-$value & $P(>|z|)$ \\
			$Outcomes \sim$         & & & & \\                                     
			SoftwareRisk   &  3.365 &   1.635 & 2.058 &   0.040 \\
			Adherence       &  $-$1.559  & 1.526  &  $-$1.022  &  0.307\\
			AssetValue     &   $-0$.619  &  0.291  &  $-$2.125 &   0.034\\
			$SoftwareRisk \sim$        & & & & \\                                  
			Adherence     &    1.56 &   1.04  &  1.505 &   0.132\\
			Covariances:  & & & & \\  
			%& Estimate & Std.Err & z$-$value & $P(>|z|)$ \\
			$AssetValue \sim\sim$          & & & & \\                                 
			Adherence     &    $-$0.024 &   0.019  &  $-$.255 &   0.209\\
		\end{tabular}
	\end{center}
\end{table}


\begin{table}
	\begin{center}	
		\caption{NVD-CII Combined, Net Data}
		\label{tab:results_combined2}
		\begin{tabular}{l|rrrr}
			\\[-1.8ex]\hline 
			\hline \\[-1.8ex] 
			\textit{Latent Variables}: &  & Combined & No Net & \\  
			$\sim$ Measured variables& Estimate & Std.Err & z$-$value & $P(>|z|)$ \\
			\hline \\[-1.8ex]
			$SoftwareRisk =\sim$  & & & & \\                                   
			cvss\_ccss\_vctr   & 1.000 & &  & \\                             
			cvss\_ccss\_cmpl &  $-$0.224 &   0.272 & 0.822 &   0.411\\
			cvss\_auth     &   $-$0.1035  &  0.114  & $-$906   & 0.365\\
			total\_code\_lns  &  7.27 &   3.054 &   2.381 &   0.017\\
			TeamSize        &  8.448   & 3.092  & 2.732   & 0.006\\
			LanguageRisk    &  $-$0.976  &  0.645   & $-$1.513   & 0.130\\ 
			& & & & \\  
			$AssetValue =\sim$     & & & & \\                                    		
			cvss\_conf\_mpct  &  1.000  &	&	&                  \\
			cvss\_intg\_mpct  &  0.743  &  0.129 &   5.770  &  0.000\\
			cvss\_aval\_mpct  &  0.481 &   0.110 &   4.364 &   0.000\\
			package\_pplrty  &  0.378 &   1.099  &  0.344 &   0.731\\	
			& & & & \\  
			$Outcomes =\sim$    & & & & \\                                     
			logCVECount     &  1.000  & & & \\                          
			& & & & \\  
			$Adherence =\sim$   & & & & \\                                      
			adherence    &     1.000        & & & \\    
			twlv\_mnth\_cnt\_  &  3.852  &  0.951  &  4.051 &   0.000\\
			CodeComments    &  $-$0.486  &  0.353  &  $-$1.376  &  0.1695\\
			TeamActivity    &  0.123  &  0.352 &   0.350 &   0.726\\	
			
			Regressions:  & & & & \\  
			%& Estimate & Std.Err & z$-$value & $P(>|z|)$ \\
			$Outcomes \sim$         & & & & \\                                     
			SoftwareRisk   &  $-$6.314 &   4.033 & $-$1.566 &   0.117 \\
			Adherence       &  4.210  & 2.340  &  1.80  &  0.072\\
			AssetValue     &   $-0$1.312  &  0.418  &  $-$3.142 &   0.002\\
			$SoftwareRisk \sim$        & & & & \\                                  
			Adherence     &    -0.003 &   0.035  &  -0.086 &   0.931\\
			Covariances:  & & & & \\  
			%& Estimate & Std.Err & z$-$value & $P(>|z|)$ \\
			$AssetValue \sim\sim$          & & & & \\                                 
			Adherence     &    $-$0.003 &   0.035  &  $-$.086 &   0.931\\
		\end{tabular}
	\end{center}
\end{table}