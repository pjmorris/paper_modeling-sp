\section{Introduction}
\label{sec:intro}
In February 2017, security researchers discovered a programming bug in an HTML parser at Cloudflare\footnote{https://blog.cloudflare.com/incident-report-on-memory-leak-caused-by-cloudflare-parser-bug/}. The parser was sending decrypted encryption keys and passwords along with ordinary web requests, resulting in unencrypted sensitive data being cached across the Internet. While the programming bug, now known as `Cloudbleed'~\footnote{https://blog.cloudflare.com/quantifying-the-impact-of-cloudbleed}, was on a single line of code, the bug had widespread effects because of the code's context, running on many servers, and sending sensitive results to many clients. The example of Cloudbleed suggests that software development teams must consider software's environment and data as well as the technical characteristics of their software when evaluating their software's security. 

The IEEE defines security~\cite{ieee1990glossary} as 'all aspects related to defining, achieving, and maintaining confidentiality, integrity, availability, non-repudiation, accountability, authenticity, and reliability of a system.' Several lists of practices for specifying how a development team defines, achieves, and maintains the various security properties in the software the team produces have been published. Example lists include the Building Security in Maturity Model ~\cite{mcgraw2006software}
(BSIMM), the Microsoft Security Development Lifecycle ~\cite{howard2009security} (SDL), the Software Assurance Forum for Excellence in Code~\cite{simpson2013fundamental} (SAFECode), and the Open Web Application Security Project (OWASP) Software Security Assurance Process ~\cite{martinez2014ssap} (SSAP). While defining how a system will support the various security properties lies within the control of the development team, achieving and maintaining security also depends on factors outside of the development team's direct control. Two pieces of software may have identical technical characteristics (e.g. same language, same size, same complexity) but different security outcomes as a consequence of the uses to which the software is put. For example, between a game level editor and a database system with identical technical characteristics, the database system managing credit card data may be more prone to attack, and to manifest vulnerabilities~\footnote{Following Krsul~\cite{krsul1998software} and Ozment~\cite{ozment2007vulnerability}, we define a vulnerability as â€œan instance of a mistake in the specification, development, or configuration of software such that its execution can violate the explicit or implicit security policy.}, than the game level editor managing custom game levels. 

Multiple software technical characteristics contribute to software vulnerabilities, for example, code size ~\cite{alhazmi2007measuring}, code churn ~\cite{shin2011evaluating,meneely2013when}, and language ~\cite{ray2014a}. Similarly, multiple software usage characteristics contribute to software vulnerabilities, for example, access to sensitive data~\footnote{http://heartbleed.com/}, management of financial information~\cite{harris2014for}, and the presence of a piece of software on large numbers of machines~\footnote{http://www.cert.org/historical/advisories/CA-2001-19.cfm} have all been associated with software vulnerabilities. Finally, the practices a development team chooses to apply, and the degree to which they adhere to those practices, affects the degree of security provided by their software. As an example of a specific practice, teams may apply analysis tools like static analyzers and fuzzers to detecting code flaws before release. However, considerations like whether the team has been trained to use the tools, whether the tools are regularly or occasionally applied, and whether the tool guidance is required to be applied to the code will affect how effective the tools are in achieving security properties.  Two teams that use the same set of tools may differ in outcomes due to their degree of adherence to the practice of applying security tooling to their software. Development teams and researchers could benefit from a comprehensive picture of the factors underlying software security concerns.

\textit{The goal of this research is to support researchers and practitioners in measuring the effect of software technical characteristics, software usage characteristics, and security practice adherence on software security outcomes by building and evaluating an explanatory model using structural equation modeling.}

We propose and evaluate a model, the \ModelName (\ModelAbbr), for quantifying security practice use and outcomes during software development. The four constructs of our model are:
\begin{enumerate}
	\item Software Risk - measures of technical characteristics of the software that have been shown to be associated with vulnerabilities and defects;	
	\item Asset Impact - measures of software usage characteristics associated with the value an attacker will find in conducting a successful attack;
	\item Practice Adherence (Adherence) - measures of the development team's security assurance efforts; 
	\item Security Outcomes (Outcomes) - measures of security-related indications associated with a piece of software over the course of the software's life cycle.
\end{enumerate}

We hypothesize that the four constructs are related as follows:
\begin{itemize}
	\item \textbf{H1} Asset Impact is associated with negative Security Outcomes
	\item \textbf{H2} Software Risk is associated with negative Security Outcomes
	\item \textbf{H3} Practice Adherence is associated with Software Risk 	
\end{itemize}
	
To evaluate the model and our hypotheses, we conduct two quantitative studies of the construct relationships, applying data from the National Vulnerability Database (NVD) and the Core Infrastructure Initiative (CII) Census to test our hypotheses.
  
Our contributions include:
\begin{itemize}
\item An explanatory model for how software development security practice use and security risk affect security outcomes;
\item Two case study evaluations on how our model fits collected data from software projects, and
\item a set of guidelines for applying our model to new software projects. 
\end{itemize}
The remainder of this paper is organized as follows:  Section \ref{sec:background} discusses background and related work. Section \ref{sec:model} presents an overview of our model and its underlying hypotheses. Section \ref{sec:methodology} presents our study methodology. Section \ref{sec:evaluation} presents the case studies and their results. Section \ref{sec:discussion} discuses the measurement results. Section \ref{sec:limitations} presents our study limitations. Section \ref{sec:conclusion} presents our conclusion.