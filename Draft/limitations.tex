\section{Limitations}
\label{sec:limitations}

We now discuss threats to validity of our study.

Our two datasets represent thousands of open source and commercial software projects. However, each dataset represents a restricted subset of software development projects, where the NVD dataset is constrained to projects with CVE records, and the OpenHub dataset is constrained to open source projects as chosen by the site's administrators. Our results are constrained to open source projects reported on by OpenHub that also have vulnerabilities reported in the NVD. Generalizing to proprietary projects, and to projects that have security vulnerabilities reported by other means, and to projects that do not have vulnerabilities will require alternate data sources.     

Kaminsky~\cite{kaminsky2011showing} critiqued the NVD data, pointing out that the existence of a vulnerability record is more indicative of reporter and finder interest in the software than of the software's quality. The strength of the effect of user\_count on Outcome shown in our analysis offers empirical evidence for Kaminsky's concern. We view reporter and finder interest as indicative of the kind of usage risk we seek to measure, distinct from software quality. Further work comparing software quality between samples of non-NVD projects and NVD projects is needed to establish the strength of the effect of reporter and finder interest, and its effect on usage risk. 

Use of the NVD vulnerability counts is a limitation, as they are externally reported and may understate the presence of security issues. Where software development teams track vulnerabilities and related security issues internally, that data can be used to increase the model's accuracy.

The variety of factors involved in security measurement suggest that further investigation is necessary. Complete validation of the model would require use of a variety of frameworks, metrics, and data sources to evaluate the constructs and their relationships. That said, we used two independent data sources, increasing confidence in correlations found in the data sets. 

In terms of construct validity, we propose a structural model of factors we believe to be relevant, and a measurement model based on the literature, but we leave room for augmenting the existing set of factors and the measurements taken on those factors. The analytical tools of SEM provide diagnostics to check for residual error and modification potential, enabling iteration over the structural and measurement models to account for additional factors in the model. 
%[Add refs to Mell/Scarfone Improving CVSS and An Analysis of CVSS scoring, add discussion of limits of NVD use of web forms for reporting vulns/cvss.]
  
The two datasets we used each contain subsets of the variables we theorize are necessary to assess security posture. We expect that the missing variables influence both the relative measures of each factor, and of the relationships between each factor. 

Statistical model-building in software engineering often uses Bayesian Belief Networks rather than SEM, e.g. Fenton~\cite{fenton2012risk}. Judea Pearl has claimed the two techniques are essentially identical, preferring SEM when the research question if of the form `What factors determine the value of this variable?' - \footnote{\url{http://causality.cs.ucla.edu/blog/index.php/2012/12/07/on-structural-equations-versus-causal-bayes-networks/}} We view our task in terms of determining the factors behind the values of the modeled variables, leading us to cast the model in terms of SEM.