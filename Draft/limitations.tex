\section{Limitations}
\label{sec:limitations}

We now discuss threats to validity of our study.

Our two datasets represent thousands of open source and commercial software projects. However, each dataset represents a restricted subset of software development projects, where the NVD dataset is constrained to projects with CVE records, and the CII dataset is constrained to Debian packages that meet the CII team's criteria. We require project data with fewer constraints to assess how generalizable our results are.    

Kaminsky~\cite{kaminsky2011showing} critiqued the NVD data, pointing out that the existence of a vulnerability record is more indicative of reporter and finder interest in the software more than of the software's quality. We view reporter and finder interest as indicative of the kind of asset risk we seek to measure, distinct from software quality. Further work comparing software quality between samples of non-NVD projects and NVD projects is needed to establish the strength of the effect of reporter and finder interest, and its effect on asset risk.

The variety of factors involved in security measurement suggest that further investigation is necessary. Complete validation of the model would require use of a variety of frameworks and data sources to evaluate the constructs and their relationships. That said, we used two independent data sources, increasing confidence in correlations found in both data sets. 

In terms of construct validity, we propose a structural model of factors we believe to be relevant, and a measurement model based on the literature, but we leave room for augmenting the existing set of factors and the measurements taken on those factors. The analytical tools of SEM provide diagnostics to check for residual error and modification potential, enabling iteration over the structural and measurement models to account for unforeseen factors in the model. 
%[Add refs to Mell/Scarfone Improving CVSS and An Analysis of CVSS scoring, add discussion of limits of NVD use of web forms for reporting vulns/cvss.]
  
The two datasets we used each contain subsets of the variables we theorize are necessary to assess security posture. We expect that the missing variables influence both the relative measures of each factor, and of the relationships between each factor. 

Statistical model-building in software engineering often uses Bayesian Belief Networks rather than SEM, e.g. [cite Fenton]. Judea Pearl has claimed the two techniques are essentially identical, preferring SEM when the research question if of the form `What factors determine the value of this variable' - \footnote{\url{http://causality.cs.ucla.edu/blog/index.php/2012/12/07/on-structural-equations-versus-causal-bayes-networks/}} We view our task in terms of determining the factors behind the values of the modeled variables, leading us to cast the model in terms of SEM.